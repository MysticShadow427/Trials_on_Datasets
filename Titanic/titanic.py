# -*- coding: utf-8 -*-
"""titanic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BK2v-twXjMElkx9Kso-0IYqKiHjd3PAH
"""

from google.colab import drive
drive.mount("/content/gdrive")
import os
os.chdir("gdrive/My Drive/Titanic")

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt 
from sklearn.linear_model import LogisticRegression 
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV

train_df=pd.read_csv('train.csv')
train_df.head()

print('Columns are:')
print(train_df.columns)
print('******************')
print('Shape is:')
print(train_df.shape)
print('******************')
print('Info:')
print(train_df.info())
print('******************')
print('Describe:')
print(train_df.describe())

train_df=train_df.drop('Name',axis=1)
train_df

train_df=train_df.drop('Ticket',axis=1)
train_df

#checking for null
train_df.isnull().sum()/len(train_df.index)

#checking for duplicate rows
train_df.duplicated()

"""<pre> There are lots of NaN values,we will do imputation to handle them.We will replace them by the mean along axis. </pre>"""

from sklearn.impute import SimpleImputer
imp=SimpleImputer(copy=False)
train_df_dup=train_df
train_df_dup

# imp.fit(train_df_dup)
# imp.transform(train_df_dup)
# train_df_dup.fillna(train_df_dup.mean())
# train_df_dup
train_df=train_df.drop('Cabin',axis=1)

train_df

# train_df_dup.fillna(train_df_dup.mean())
# train_df_dup
train_df['Age'].fillna((train_df['Age'].mean()), inplace=True)
train_df

"""<p>Deleted 'Name','Cabin','PassengerId' and 'Ticket' column.</p>
<p>None of the column is null.</p>
<p>No duplicates.</p>
<p>Imputed NaN values with Mean of the column.</p>
"""

train_df['Survived'].value_counts()

train_df['Embarked'].value_counts()

#converting categorical to numeric
train_df['Sex']=train_df['Sex'].astype('category')
train_df['Embarked']=train_df['Embarked'].astype('category')

cat_features = train_df.select_dtypes(['category']).columns
# print(cat_features)
train_df[cat_features]= train_df[cat_features].apply(lambda x: x.cat.codes)
train_df

train_df=train_df.drop('PassengerId',axis=1)
x=train_df.iloc[:, 1:]
y=train_df['Survived']
x

#splitting data
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.7,test_size = 0.3, random_state = 1234)

print('Shape of X Train:',x_train.shape)
print('*****************')
print('Shape of X Test: ',x_test.shape)

#standardizing the data 
from sklearn.preprocessing import StandardScaler
columns = x_train.columns
scalerx = StandardScaler()
x_train_scaled = scalerx.fit_transform(x_train)
x_train_scaled = pd.DataFrame(x_train_scaled, columns = columns)

x_test_scaled = scalerx.transform(x_test)
x_test_scaled = pd.DataFrame(x_test_scaled, columns = columns)

"""# EDA and Visualizations"""

#plotting pair plots as there very small number of features
sns.set_style('whitegrid')
sns.pairplot(train_df,hue='Survived')
plt.show()

"""<p> As we can see there is lots of overlapping so we cannot build a threshold for a simple model.All the features are now important."""

#trying to visualize using t-SNE
from sklearn.manifold import TSNE

model=TSNE(n_components=2,perplexity=5,random_state=0,n_iter=3000)
# configuring the parameteres
# the number of components = 2
# default perplexity = 30
# default learning rate = 200
# default Maximum number of iterations for the optimization = 1000
tsne_data=model.fit_transform(x)

#creating a new data frame which help us in ploting the result data
# creating a new data frame which help us in ploting the result data
tsne_data = np.vstack((tsne_data.T, y)).T
tsne_df = pd.DataFrame(data=tsne_data, columns=("Dim_1", "Dim_2", "label"))

# Ploting the result of tsne
sns.FacetGrid(tsne_df, hue="label", size=6).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()
plt.show()

#pca
from sklearn.decomposition import IncrementalPCA
pca=IncrementalPCA(n_components=2)
x_train_pca = pca.fit_transform(x_train_scaled)

x_test_pca = pca.transform(x_test_scaled)

plt.figure(figsize = (20, 6))
plt.subplot(1, 2, 1)
plt.scatter(x_train_pca[:,0], x_train_pca[:,1], c = y_train)
plt.xlabel('Training 1st Principal Component')
plt.ylabel('Training 2nd Principal Component')
plt.title('Training Set Scatter Plot with labels indicated by colors, (0)-Violet,(1)-Yellow')
plt.subplot(1, 2, 2)
plt.scatter(x_test_pca[:,0], x_test_pca[:,1], c = y_test)
plt.xlabel('Test 1st Principal Component')
plt.ylabel('Test 2nd Principal Component')
plt.title('Test Set Scatter Plot with labels indicated by colors, (0)-Violet (1)-Yellow')
plt.show()

"""<p> As we can see data from tsne and pca data is  not that linearly seperable</p>

#Models
"""

#fitting logistic regression using cross validation
tuned_parameters = [{'C': [10**-4, 10**-2, 10**0, 10**2, 10**4]}]
#Using GridSearchCV
model = GridSearchCV(LogisticRegression(), tuned_parameters, scoring = 'accuracy', return_train_score=True,verbose=3,cv=3)
model.fit(x_train_scaled, y_train)

print('Best Estimator is:',model.best_estimator_)
print('Accuracy is:',model.score(x_test_scaled, y_test))

from sklearn import metrics
model=LogisticRegression(C=0.01).fit(x_train,y_train)
y_train_pred = model.predict(x_train)

y_test_pred = model.predict(x_test)

print('Training Accuracy of the Model: ', metrics.accuracy_score(y_train, y_train_pred))
print('Test Accuracy of the Model: ', metrics.accuracy_score(y_test, y_test_pred))


print('Training Precision of the Model: ', metrics.precision_score(y_train, y_train_pred))
print('Test Precision of the Model: ', metrics.precision_score(y_test, y_test_pred))

# knn with simple cross validation
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
# split the data set into train and test
X_1, X_test, y_1, y_test = train_test_split(x, y, test_size=0.3, random_state=0)

# split the train data set into cross validation train and cross validation test
X_tr, X_cv, y_tr, y_cv = train_test_split(X_1, y_1, test_size=0.3)

for i in range(1,30,2):
    # instantiate learning model (k = 30)
    knn = KNeighborsClassifier(n_neighbors=i)

    # fitting the model on crossvalidation train
    knn.fit(X_tr, y_tr)

    # predict the response on the crossvalidation train
    pred = knn.predict(X_cv)

    # evaluate CV accuracy
    acc = accuracy_score(y_cv, pred, normalize=True) * float(100)
    print('\nCV accuracy for k = %d is %d%%' % (i, acc))
    
# knn = KNeighborsClassifier(1)
# knn.fit(X_tr,y_tr)
# pred = knn.predict(X_test)
# acc = accuracy_score(y_test, pred, normalize=True) * float(100)
# print('\n****Test accuracy for k = 1 is %d%%' % (acc))

knn = KNeighborsClassifier(n_neighbors=23,weights='distance')
knn.fit(X_tr,y_tr)
pred = knn.predict(X_test)
acc = accuracy_score(y_test, pred, normalize=True) * float(100)
print('\n****Test accuracy for k = 23 is %d%%' % (acc))

#svm
from sklearn.svm import SVC
param_grid= {'C': [10**-4,10**-3, 10**-2,10**-1,10**0,10, 10**2, 10**3,10**4],
                      'gamma':[1, 0.1, 0.01, 0.001, 0.0001],
                      'kernel': ['rbf']}
grid_search=GridSearchCV(SVC(),param_grid,scoring='accuracy',return_train_score=True,verbose=3,cv=5)
grid_search.fit(X_tr, y_tr)


print('Best Estimator is:',grid_search.best_estimator_)
print('Accuracy is:',grid_search.score(X_test, y_test))

test_df=pd.read_csv('test.csv')
test_df

train_df

test_df=test_df.drop('Name',axis=1)
test_df=test_df.drop('Ticket',axis=1)
test_df=test_df.drop('Cabin',axis=1)
test_df

test_df['Sex']=test_df['Sex'].astype('category')
test_df['Embarked']=test_df['Embarked'].astype('category')

cat_features_test = test_df.select_dtypes(['category']).columns
# print(cat_features)
test_df[cat_features_test]= test_df[cat_features_test].apply(lambda x: x.cat.codes)
test_df

x_test_df=test_df.iloc[:, 1:]
y_test_df=test_df['PassengerId']
x_test_df

y_test_df

columns_test=x_test_df.columns
x_test_df_scaled = scalerx.transform(x_test_df)
x_test_df_scaled = pd.DataFrame(x_test_df_scaled, columns = columns_test)
x_test_df_scaled

x_test_df_scaled['Age'].fillna((x_test_df_scaled['Age'].mean()), inplace=True)
x_test_df_scaled['Pclass'].fillna((x_test_df_scaled['Pclass'].mean()), inplace=True)
x_test_df_scaled['SibSp'].fillna((x_test_df_scaled['SibSp'].mean()), inplace=True)
x_test_df_scaled['Parch'].fillna((x_test_df_scaled['Parch'].mean()), inplace=True)
x_test_df_scaled['Fare'].fillna((x_test_df_scaled['Fare'].mean()), inplace=True)
x_test_df_scaled['Embarked'].fillna((x_test_df_scaled['Embarked'].mean()), inplace=True)
x_test_df_scaled['Sex'].fillna((x_test_df_scaled['Sex'].mean()), inplace=True)
#x_test_df_scaled
print(np.any(np.isnan(x_test_df_scaled)))
print(np.all(np.isfinite(x_test_df_scaled)))
print(x_test_df_scaled['Age'].value_counts())
print(x_test_df_scaled['Pclass'].value_counts())
print(x_test_df_scaled['Embarked'].value_counts())
print(x_test_df_scaled['Sex'].value_counts())
print(x_test_df_scaled['Parch'].value_counts())
print(x_test_df_scaled['Fare'].value_counts())

svc=SVC(C=10000, gamma=0.0001,kernel='rbf')
svc.fit(x_train_scaled, y_train)

y_hat=svc.predict(x_test_df_scaled)
print(svc.score(x_test_df_scaled,y_test_df))
print(y_hat)

y_test_df

final_df=(pd.DataFrame({
                'PassengerId':y_test_df,
                'Survived':y_hat
}))
final_df

final_df.to_csv('submissions.csv')

final_df.to_csv('submission.csv',index=False)

final_df

tp=pd.read_csv('submission.csv')
tp #ye sahi hua index=false karke

tp_df=pd.read_csv('submissions.csv')
tp_df

from sklearn.ensemble import RandomForestClassifier
clf=RandomForestClassifier()
clf.fit(x_train_scaled, y_train)
y_hat_rn=svc.predict(x_test_df_scaled)
print(y_hat_rn)

final_df_rn=(pd.DataFrame({
                'PassengerId':y_test_df,
                'Survived':y_hat_rn
}))
final_df_rn

final_df_rn.to_csv('submission_rn.csv',index=False)

final_df_rd=(pd.DataFrame({
                'PassengerId':y_test_df,
                'Survived':y_hat_rn
}))
final_df_rd

final_df_rd.to_csv('submission_rd.csv',index=False)